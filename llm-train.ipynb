{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86946,"databundleVersionId":10131489,"sourceType":"competition"},{"sourceId":9041096,"sourceType":"datasetVersion","datasetId":5450606}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install accelerate==0.33.0\n!pip install bitsandbytes==0.43.3\n!pip install peft==0.12.0 \n!pip install transformers==4.44.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:25:35.431725Z","iopub.execute_input":"2024-12-02T14:25:35.432470Z","iopub.status.idle":"2024-12-02T14:26:25.630394Z","shell.execute_reply.started":"2024-12-02T14:25:35.432432Z","shell.execute_reply":"2024-12-02T14:26:25.629391Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting accelerate==0.33.0\n  Downloading accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: numpy<2.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0) (2.4.0)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0) (0.25.1)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0) (0.4.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.33.0) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.33.0) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.33.0) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.33.0) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.33.0) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate==0.33.0) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.33.0) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.33.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.33.0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.33.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.33.0) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.33.0) (1.3.0)\nDownloading accelerate-0.33.0-py3-none-any.whl (315 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.34.2\n    Uninstalling accelerate-0.34.2:\n      Successfully uninstalled accelerate-0.34.2\nSuccessfully installed accelerate-0.33.0\nCollecting bitsandbytes==0.43.3\n  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes==0.43.3) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes==0.43.3) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.3) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.3) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.3) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.3) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.3) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.3) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes==0.43.3) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes==0.43.3) (1.3.0)\nDownloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.43.3\nCollecting peft==0.12.0\n  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.12.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.12.0) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.12.0) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.12.0) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.12.0) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft==0.12.0) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft==0.12.0) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.12.0) (0.33.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.12.0) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.12.0) (0.25.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.12.0) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.12.0) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.12.0) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.12.0) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.12.0) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.12.0) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.12.0) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.12.0) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.12.0) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.12.0) (0.20.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.12.0) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.12.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.12.0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.12.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.12.0) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.12.0) (1.3.0)\nDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.12.0\nCollecting transformers==4.44.0\n  Downloading transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.44.0) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers==4.44.0) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.44.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.44.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.44.0) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.44.0) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.44.0) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.44.0) (0.4.5)\nCollecting tokenizers<0.20,>=0.19 (from transformers==4.44.0)\n  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.44.0) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.0) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.0) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.44.0) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.44.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.44.0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.44.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.44.0) (2024.8.30)\nDownloading transformers-4.44.0-py3-none-any.whl (9.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.20.0\n    Uninstalling tokenizers-0.20.0:\n      Successfully uninstalled tokenizers-0.20.0\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.45.1\n    Uninstalling transformers-4.45.1:\n      Successfully uninstalled transformers-4.45.1\nSuccessfully installed tokenizers-0.19.1 transformers-4.44.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n\nVER=157\n\n# FINAL SOLUTION IS USE_QLORA=FALSE, TRAIN_100_PERCENT=TRUE, ADD_33K=TRUE, DEBUG=FALSE\nUSE_QLORA = True\nTRAIN_100_PERCENT = False\nADD_33K = False\nDEBUG = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:26:25.633081Z","iopub.execute_input":"2024-12-02T14:26:25.634016Z","iopub.status.idle":"2024-12-02T14:26:25.639133Z","shell.execute_reply.started":"2024-12-02T14:26:25.633962Z","shell.execute_reply":"2024-12-02T14:26:25.637978Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nimport copy\nfrom dataclasses import dataclass\n\nimport numpy as np\nimport torch\nfrom datasets import Dataset\nfrom transformers import (\n    BitsAndBytesConfig,\n    Gemma2ForSequenceClassification,\n    GemmaTokenizerFast,\n    Gemma2Config,\n    PreTrainedTokenizerBase, \n    EvalPrediction,\n    Trainer,\n    TrainingArguments,\n    DataCollatorWithPadding,\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\nfrom sklearn.metrics import log_loss, accuracy_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:26:25.640733Z","iopub.execute_input":"2024-12-02T14:26:25.641399Z","iopub.status.idle":"2024-12-02T14:26:44.780741Z","shell.execute_reply.started":"2024-12-02T14:26:25.641337Z","shell.execute_reply":"2024-12-02T14:26:44.779856Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"@dataclass\nclass Config:\n    output_dir: str = f\"output-{VER}\"\n    checkpoint: str = \"/kaggle/input/gemma2-9b-it-fp16\"  \n    max_length: int = 2048\n    n_splits: int = 5\n    fold_idx: int = 0\n    optim_type: str = \"adamw_8bit\"\n    per_device_train_batch_size: int = 2\n    gradient_accumulation_steps: int = 4  # global batch size is 8 \n    per_device_eval_batch_size: int = 4\n    n_epochs: int = 1\n    freeze_layers: int = 0 # there're 42 layers in total, we don't add adapters to the first 16 layers\n    lr: float = 2e-4\n    warmup_steps: int = 20\n    lora_r: int = 64\n    lora_alpha: float = 4 \n    lora_dropout: float = 0.05\n    lora_bias: str = \"none\"\n    \nconfig = Config()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:26:44.781833Z","iopub.execute_input":"2024-12-02T14:26:44.782375Z","iopub.status.idle":"2024-12-02T14:26:44.788945Z","shell.execute_reply.started":"2024-12-02T14:26:44.782346Z","shell.execute_reply":"2024-12-02T14:26:44.788045Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir = f\"output-{VER}\",\n    overwrite_output_dir=True,\n    report_to=\"none\",\n    num_train_epochs=config.n_epochs,\n    per_device_train_batch_size=config.per_device_train_batch_size,\n    gradient_accumulation_steps=config.gradient_accumulation_steps,\n    per_device_eval_batch_size=config.per_device_eval_batch_size,\n    logging_steps=10,\n    eval_strategy=\"epoch\",\n    save_strategy=\"no\", # don't save any checkpoints\n    #save_steps=200,\n    optim=config.optim_type,\n    fp16=True, \n    #bf16=False,\n    learning_rate=config.lr,\n    warmup_steps=config.warmup_steps,\n\n    #gradient_checkpointing=True, # this doesn't work correctly for some reason\n\n    #logging_first_step=True,\n    #lr_scheduler_type='linear', # \"cosine\" or \"linear\" or \"constant\" (default is linear)\n    metric_for_best_model='log_loss',\n    greater_is_better=False,  \n    #save_total_limit=4,\n    #load_best_model_at_end=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:26:44.791511Z","iopub.execute_input":"2024-12-02T14:26:44.791839Z","iopub.status.idle":"2024-12-02T14:26:44.931874Z","shell.execute_reply.started":"2024-12-02T14:26:44.791793Z","shell.execute_reply":"2024-12-02T14:26:44.930956Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=config.lora_r,\n    lora_alpha=config.lora_alpha,\n    # only target self-attention\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\",\n                    \"down_proj\",\"up_proj\",\"o_proj\",\"gate_proj\"],\n    layers_to_transform=[i for i in range(42) if i >= config.freeze_layers],\n    lora_dropout=config.lora_dropout,\n    bias=config.lora_bias,\n    task_type=TaskType.SEQ_CLS,\n    modules_to_save=[\"score\",\"classifier_head1\", \"classifier_head2\"]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:26:44.932955Z","iopub.execute_input":"2024-12-02T14:26:44.933227Z","iopub.status.idle":"2024-12-02T14:26:44.938066Z","shell.execute_reply.started":"2024-12-02T14:26:44.933200Z","shell.execute_reply":"2024-12-02T14:26:44.937187Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"tokenizer = GemmaTokenizerFast.from_pretrained(config.checkpoint)\ntokenizer.add_eos_token = True  # We'll add <eos> at the end\ntokenizer.padding_side = \"right\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:26:44.939055Z","iopub.execute_input":"2024-12-02T14:26:44.939324Z","iopub.status.idle":"2024-12-02T14:26:45.845391Z","shell.execute_reply.started":"2024-12-02T14:26:44.939278Z","shell.execute_reply":"2024-12-02T14:26:45.844692Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"qlora = {}\nif USE_QLORA:\n    from transformers import BitsAndBytesConfig\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit = True,\n        bnb_4bit_quant_type = \"nf4\", #nf4 or fp4\n        bnb_4bit_use_double_quant = False,\n        bnb_4bit_compute_dtype=torch.float16,\n        llm_int8_skip_modules = [\"score\",\"classifier_head1\", \"classifier_head2\"]\n    )\n    qlora['quantization_config'] = bnb_config\n    print(\"Using QLoRA\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:26:45.846395Z","iopub.execute_input":"2024-12-02T14:26:45.846648Z","iopub.status.idle":"2024-12-02T14:26:45.852938Z","shell.execute_reply.started":"2024-12-02T14:26:45.846623Z","shell.execute_reply":"2024-12-02T14:26:45.852119Z"}},"outputs":[{"name":"stdout","text":"Using QLoRA\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import Gemma2ForSequenceClassification, Gemma2Config\n\nclass CustomGemma2ForSequenceClassification(Gemma2ForSequenceClassification):\n    def __init__(self, config, num_labels_head1=60, num_labels_head2=60):\n        super().__init__(config)\n        self.num_labels_head1 = num_labels_head1\n        self.num_labels_head2 = num_labels_head2\n        self.classifier_head1 = nn.Linear(config.hidden_size, num_labels_head1, bias=False)\n        self.classifier_head2 = nn.Linear(config.hidden_size, num_labels_head2, bias=False)\n\n    def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):\n        device = input_ids.device\n\n        if labels is not None:\n            labels = labels.to(device)\n            outputs = super().forward(input_ids, attention_mask=attention_mask, labels=labels[:, 0], output_hidden_states=True)\n        else:\n            outputs = super().forward(input_ids, attention_mask=attention_mask)\n\n        last_token_indices = (torch.sum(attention_mask, dim=1) - 1).to(device)\n        last_token_outputs = outputs.hidden_states[-1].to(device)[\n            torch.arange(outputs.hidden_states[-1].shape[0], device=device), last_token_indices]\n\n        outputs_head1 = self.classifier_head1(last_token_outputs).to(device)\n        outputs_head2 = self.classifier_head2(last_token_outputs).to(device)\n\n        if labels is not None:\n            labels_head1 = labels[:, 1].to(device)\n            labels_head2 = labels[:, 2].to(device)\n            \n            loss_head1 = nn.CrossEntropyLoss()(outputs_head1, labels_head1)\n            loss_head2 = nn.CrossEntropyLoss()(outputs_head2, labels_head2)\n            loss = outputs.loss.to(device) + 0.1 * loss_head1 + 0.1 * loss_head2\n            return {\"loss\": loss, \"logits\": (outputs.logits, outputs_head1, outputs_head2)}\n        else:\n            return {\"logits\": (outputs.logits, outputs_head1, outputs_head2)}\n\nconfig2 = Gemma2Config.from_pretrained(config.checkpoint)\nconfig2.num_labels = 2\nmodel = CustomGemma2ForSequenceClassification.from_pretrained(\n    config.checkpoint,\n    config=config2,\n    num_labels_head1=60,\n    num_labels_head2=60,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    **qlora\n)\n\nmodel.config.use_cache = False\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, lora_config)\nmodel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:26:45.854379Z","iopub.execute_input":"2024-12-02T14:26:45.854760Z","iopub.status.idle":"2024-12-02T14:28:28.618804Z","shell.execute_reply.started":"2024-12-02T14:26:45.854723Z","shell.execute_reply":"2024-12-02T14:28:28.617968Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3c98ff91a504f8e848eb81081a58443"}},"metadata":{}},{"name":"stderr","text":"Some weights of CustomGemma2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/gemma2-9b-it-fp16 and are newly initialized: ['classifier_head1.weight', 'classifier_head2.weight', 'score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"PeftModelForSequenceClassification(\n  (base_model): LoraModel(\n    (model): CustomGemma2ForSequenceClassification(\n      (model): Gemma2Model(\n        (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n        (layers): ModuleList(\n          (0-41): 42 x Gemma2DecoderLayer(\n            (self_attn): Gemma2SdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3584, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3584, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3584, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3584, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3584, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3584, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=3584, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=3584, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): Gemma2RotaryEmbedding()\n            )\n            (mlp): Gemma2MLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3584, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3584, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3584, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3584, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=14336, out_features=3584, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=14336, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=3584, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): PytorchGELUTanh()\n            )\n            (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n            (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n            (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n            (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n          )\n        )\n        (norm): Gemma2RMSNorm((3584,), eps=1e-06)\n      )\n      (score): ModulesToSaveWrapper(\n        (original_module): Linear(in_features=3584, out_features=2, bias=False)\n        (modules_to_save): ModuleDict(\n          (default): Linear(in_features=3584, out_features=2, bias=False)\n        )\n      )\n      (classifier_head1): ModulesToSaveWrapper(\n        (original_module): Linear(in_features=3584, out_features=60, bias=False)\n        (modules_to_save): ModuleDict(\n          (default): Linear(in_features=3584, out_features=60, bias=False)\n        )\n      )\n      (classifier_head2): ModulesToSaveWrapper(\n        (original_module): Linear(in_features=3584, out_features=60, bias=False)\n        (modules_to_save): ModuleDict(\n          (default): Linear(in_features=3584, out_features=60, bias=False)\n        )\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"model.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:28:28.620093Z","iopub.execute_input":"2024-12-02T14:28:28.620470Z","iopub.status.idle":"2024-12-02T14:28:28.632665Z","shell.execute_reply.started":"2024-12-02T14:28:28.620429Z","shell.execute_reply":"2024-12-02T14:28:28.631655Z"}},"outputs":[{"name":"stdout","text":"trainable params: 216,509,440 || all params: 9,458,652,672 || trainable%: 2.2890\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_parquet(\"/kaggle/input/wsdm-cup-multilingual-chatbot-arena/train.parquet\") \ndf[\"id\"] = df[\"id\"].astype(\"str\")\nprint('Competition data has shape', df.shape )\nLN = len(df)\ndf.head(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:28:28.633920Z","iopub.execute_input":"2024-12-02T14:28:28.634207Z","iopub.status.idle":"2024-12-02T14:28:31.267771Z","shell.execute_reply.started":"2024-12-02T14:28:28.634176Z","shell.execute_reply":"2024-12-02T14:28:31.266987Z"}},"outputs":[{"name":"stdout","text":"Competition data has shape (48439, 8)\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                                                  id              prompt  \\\n0  00007cff95d7f7974642a785aca248b0f26e60d3312fac...  vieš po Slovensky?   \n\n                                         response_a  \\\n0  Áno, hovorím po slovensky. Ako vám môžem pomôcť?   \n\n                                          response_b   winner     model_a  \\\n0  Áno, veď som tu! Môžem ti pomôcť s otázkami al...  model_a  o1-preview   \n\n              model_b language  \n0  reka-core-20240904   Slovak  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>winner</th>\n      <th>model_a</th>\n      <th>model_b</th>\n      <th>language</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00007cff95d7f7974642a785aca248b0f26e60d3312fac...</td>\n      <td>vieš po Slovensky?</td>\n      <td>Áno, hovorím po slovensky. Ako vám môžem pomôcť?</td>\n      <td>Áno, veď som tu! Môžem ti pomôcť s otázkami al...</td>\n      <td>model_a</td>\n      <td>o1-preview</td>\n      <td>reka-core-20240904</td>\n      <td>Slovak</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"if ADD_33K:\n    df = pd.concat([df,df2],axis=0,ignore_index=True)\nif DEBUG:\n    df = df.iloc[:64].copy()\nprint(\"We will use train data with shape\", df.shape )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:28:31.269117Z","iopub.execute_input":"2024-12-02T14:28:31.269744Z","iopub.status.idle":"2024-12-02T14:28:31.277517Z","shell.execute_reply.started":"2024-12-02T14:28:31.269702Z","shell.execute_reply":"2024-12-02T14:28:31.276661Z"}},"outputs":[{"name":"stdout","text":"We will use train data with shape (64, 8)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import numpy as np\nm1 = df.model_a.unique()\nm2 = df.model_b.unique()\nm = np.union1d(m1,m2)\nm = sorted(m)\nprint(f\"There are {len(m)} unique models:\")\n\nMAP = {x:y for x,y in zip(m,range(len(m)))}\nprint(MAP)\n\ndf.model_a = df.model_a.map(MAP).astype('int32')\ndf.model_b = df.model_b.map(MAP).astype('int32')\ndf.head(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:28:31.278673Z","iopub.execute_input":"2024-12-02T14:28:31.279026Z","iopub.status.idle":"2024-12-02T14:28:31.301407Z","shell.execute_reply.started":"2024-12-02T14:28:31.278989Z","shell.execute_reply":"2024-12-02T14:28:31.300653Z"}},"outputs":[{"name":"stdout","text":"There are 47 unique models:\n{'chatgpt-4o-latest-20240808': 0, 'chatgpt-4o-latest-20240903': 1, 'claude-3-5-sonnet-20240620': 2, 'claude-3-5-sonnet-20241022': 3, 'claude-3-haiku-20240307': 4, 'claude-3-opus-20240229': 5, 'command-r-08-2024': 6, 'command-r-plus-08-2024': 7, 'deepseek-v2.5': 8, 'gemini-1.5-flash-002': 9, 'gemini-1.5-flash-8b-001': 10, 'gemini-1.5-flash-8b-exp-0827': 11, 'gemini-1.5-flash-exp-0827': 12, 'gemini-1.5-pro-001': 13, 'gemini-1.5-pro-002': 14, 'gemini-1.5-pro-exp-0827': 15, 'gemma-2-27b-it': 16, 'gemma-2-2b-it': 17, 'gemma-2-9b-it': 18, 'gpt-4-0125-preview': 19, 'gpt-4-1106-preview': 20, 'gpt-4-turbo-2024-04-09': 21, 'gpt-4o-2024-05-13': 22, 'gpt-4o-2024-08-06': 23, 'gpt-4o-mini-2024-07-18': 24, 'grok-2-2024-08-13': 25, 'grok-2-mini-2024-08-13': 26, 'internlm2_5-20b-chat': 27, 'jamba-1.5-mini': 28, 'llama-3.1-405b-instruct-bf16': 29, 'llama-3.1-405b-instruct-fp8': 30, 'llama-3.1-70b-instruct': 31, 'llama-3.1-8b-instruct': 32, 'llama-3.1-nemotron-70b-instruct': 33, 'llama-3.2-1b-instruct': 34, 'llama-3.2-3b-instruct': 35, 'mistral-large-2407': 36, 'o1-mini': 37, 'o1-preview': 38, 'qwen-max-0919': 39, 'qwen-plus-0828': 40, 'qwen2.5-72b-instruct': 41, 'reka-core-20240904': 42, 'reka-flash-20240722': 43, 'reka-flash-20240904': 44, 'yi-lightning': 45, 'yi-lightning-lite': 46}\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                                  id              prompt  \\\n0  00007cff95d7f7974642a785aca248b0f26e60d3312fac...  vieš po Slovensky?   \n\n                                         response_a  \\\n0  Áno, hovorím po slovensky. Ako vám môžem pomôcť?   \n\n                                          response_b   winner  model_a  \\\n0  Áno, veď som tu! Môžem ti pomôcť s otázkami al...  model_a       38   \n\n   model_b language  \n0       42   Slovak  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>winner</th>\n      <th>model_a</th>\n      <th>model_b</th>\n      <th>language</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00007cff95d7f7974642a785aca248b0f26e60d3312fac...</td>\n      <td>vieš po Slovensky?</td>\n      <td>Áno, hovorím po slovensky. Ako vám môžem pomôcť?</td>\n      <td>Áno, veď som tu! Môžem ti pomôcť s otázkami al...</td>\n      <td>model_a</td>\n      <td>38</td>\n      <td>42</td>\n      <td>Slovak</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"ds = Dataset.from_pandas(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:28:31.304451Z","iopub.execute_input":"2024-12-02T14:28:31.305006Z","iopub.status.idle":"2024-12-02T14:28:31.325700Z","shell.execute_reply.started":"2024-12-02T14:28:31.304978Z","shell.execute_reply":"2024-12-02T14:28:31.325004Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import json\n\nclass CustomTokenizer:\n    def __init__(\n        self, \n        tokenizer: PreTrainedTokenizerBase, \n        max_length: int\n    ) -> None:\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def prepare_text(self, prompts, responses_a, responses_b):\n        \n        rounds = [\n            f\"<start_of_turn>prompt\\n{prompts}<end_of_turn>\\n\"\n            +f\"<start_of_turn>response_a\\n{responses_a}<end_of_turn>\\n\"\n            +f\"<start_of_turn>response_b\\n{responses_b}<end_of_turn>\"\n        ]\n        \n        # for k in range(len(rounds)):\n        #     tmp = \"\\n\".join(rounds[k:])\n        #     if len( self.tokenizer(tmp)[\"input_ids\"] ) < self.max_length: \n        #         break\n        tmp = rounds[0]\n        \n        return tmp\n        \n    def __call__(self, batch: dict) -> dict:\n        \n        texts = [\n            self.prepare_text(p, r_a, r_b)\n            for p, r_a, r_b in zip(batch[\"prompt\"], batch[\"response_a\"], batch[\"response_b\"])\n        ]\n        \n        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)\n        labels=[]\n        for win, c, d in zip(batch[\"winner\"], \n                                   batch[\"model_a\"],batch[\"model_b\"]):\n            if win == 'model_a':\n                label = 0\n            elif win == 'model_b':\n                label = 1\n            labels.append( (label,c,d) )\n        return {**tokenized, \"labels\": labels} #, \"texts\": texts}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:28:31.326565Z","iopub.execute_input":"2024-12-02T14:28:31.326806Z","iopub.status.idle":"2024-12-02T14:28:31.333542Z","shell.execute_reply.started":"2024-12-02T14:28:31.326782Z","shell.execute_reply":"2024-12-02T14:28:31.332750Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"encode = CustomTokenizer(tokenizer, max_length=config.max_length)\nds = ds.map(encode, batched=True, num_proc=8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:28:31.334643Z","iopub.execute_input":"2024-12-02T14:28:31.335130Z","iopub.status.idle":"2024-12-02T14:28:38.327758Z","shell.execute_reply.started":"2024-12-02T14:28:31.335103Z","shell.execute_reply":"2024-12-02T14:28:38.326953Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=8):   0%|          | 0/64 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d4d7a340a2b4a16b03cfa3c386f2c57"}},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"def compute_metrics(eval_preds: EvalPrediction) -> dict:\n    preds = eval_preds.predictions\n    labels = np.array( eval_preds.label_ids )\n    \n    # Split the predictions and labels into two heads\n    preds_head1 = preds[0]\n    preds_head2 = preds[1]\n    preds_head3 = preds[2]\n    labels_head1 = labels[:,0]\n    labels_head2 = labels[:,1]\n    labels_head3 = labels[:,2]\n    \n    # Compute log loss and accuracy for each head\n    probs_head1 = torch.from_numpy(preds_head1).float().softmax(-1).numpy()\n    loss_head1 = log_loss(y_true=labels_head1, y_pred=probs_head1, labels=[x for x in range(2)])\n    acc_head1 = accuracy_score(y_true=labels_head1, y_pred=preds_head1.argmax(-1))\n    \n    probs_head2 = torch.from_numpy(preds_head2).float().softmax(-1).numpy()\n    loss_head2 = log_loss(y_true=labels_head2, y_pred=probs_head2, labels=[x for x in range(60)])\n    acc_head2 = accuracy_score(y_true=labels_head2, y_pred=preds_head2.argmax(-1))\n\n    probs_head3 = torch.from_numpy(preds_head3).float().softmax(-1).numpy()\n    loss_head3 = log_loss(y_true=labels_head3, y_pred=probs_head3, labels=[x for x in range(60)])\n    acc_head3 = accuracy_score(y_true=labels_head3, y_pred=preds_head3.argmax(-1))\n    \n    # Return the metrics for each head\n    return {\n        \"acc_classify\": acc_head1,\n        \"log_loss_classify\": loss_head1,\n        \"acc_model_a\": acc_head2,\n        \"log_loss_model_a\": loss_head2,\n        \"acc_model_b\": acc_head3,\n        \"log_loss_model_b\": loss_head3\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:28:38.328989Z","iopub.execute_input":"2024-12-02T14:28:38.329244Z","iopub.status.idle":"2024-12-02T14:28:38.337247Z","shell.execute_reply.started":"2024-12-02T14:28:38.329215Z","shell.execute_reply":"2024-12-02T14:28:38.336226Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"if TRAIN_100_PERCENT:\n    folds = [\n        (\n            [i for i in range(len(ds))], \n            [i for i in range(len(ds)) if (i % config.n_splits == fold_idx)&(i<LN)]\n        ) \n        for fold_idx in range(config.n_splits)\n    ]\n    print(\"We are training with 100% data\")\nelse:\n    folds = [\n        (\n            [i for i in range(len(ds)) if i % config.n_splits != fold_idx],\n            [i for i in range(len(ds)) if (i % config.n_splits == fold_idx)&(i<LN)]\n        ) \n        for fold_idx in range(config.n_splits)\n    ]    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:28:38.338251Z","iopub.execute_input":"2024-12-02T14:28:38.338501Z","iopub.status.idle":"2024-12-02T14:28:38.351380Z","shell.execute_reply.started":"2024-12-02T14:28:38.338477Z","shell.execute_reply":"2024-12-02T14:28:38.350713Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"train_idx, eval_idx = folds[config.fold_idx]\n\ntrainer = Trainer(\n    args=training_args, \n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=ds.select(train_idx),\n    eval_dataset=ds.select(eval_idx),\n    compute_metrics=compute_metrics,\n    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n)\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:28:38.352352Z","iopub.execute_input":"2024-12-02T14:28:38.352620Z","iopub.status.idle":"2024-12-02T14:35:59.134920Z","shell.execute_reply.started":"2024-12-02T14:28:38.352594Z","shell.execute_reply":"2024-12-02T14:35:59.133991Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6/6 06:17, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Acc Classify</th>\n      <th>Log Loss Classify</th>\n      <th>Acc Model A</th>\n      <th>Log Loss Model A</th>\n      <th>Acc Model B</th>\n      <th>Log Loss Model B</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>No log</td>\n      <td>3.254423</td>\n      <td>0.461538</td>\n      <td>2.209075</td>\n      <td>0.076923</td>\n      <td>5.105834</td>\n      <td>0.000000</td>\n      <td>5.348514</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=6, training_loss=4.134893417358398, metrics={'train_runtime': 439.3599, 'train_samples_per_second': 0.116, 'train_steps_per_second': 0.014, 'total_flos': 3365690881093632.0, 'train_loss': 4.134893417358398, 'epoch': 0.9230769230769231})"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"trainer.save_model(f\"LoRA-v{VER}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}