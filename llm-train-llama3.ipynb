{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ce745f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T15:46:13.117881Z",
     "iopub.status.busy": "2024-12-28T15:46:13.117543Z",
     "iopub.status.idle": "2024-12-28T15:47:03.727375Z",
     "shell.execute_reply": "2024-12-28T15:47:03.726485Z"
    },
    "papermill": {
     "duration": 50.617738,
     "end_time": "2024-12-28T15:47:03.729364",
     "exception": false,
     "start_time": "2024-12-28T15:46:13.111626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate==0.33.0\r\n",
      "  Downloading accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\r\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0) (21.3)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0) (5.9.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0) (6.0.2)\r\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0) (2.4.0)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0) (0.25.1)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0) (0.4.5)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.33.0) (3.15.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.33.0) (2024.6.1)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.33.0) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.33.0) (4.66.4)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.33.0) (4.12.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate==0.33.0) (3.1.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (3.1.4)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.33.0) (2.1.5)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.33.0) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.33.0) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.33.0) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.33.0) (2024.8.30)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.33.0) (1.3.0)\r\n",
      "Downloading accelerate-0.33.0-py3-none-any.whl (315 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: accelerate\r\n",
      "  Attempting uninstall: accelerate\r\n",
      "    Found existing installation: accelerate 0.34.2\r\n",
      "    Uninstalling accelerate-0.34.2:\r\n",
      "      Successfully uninstalled accelerate-0.34.2\r\n",
      "Successfully installed accelerate-0.33.0\r\n",
      "Collecting bitsandbytes==0.43.3\r\n",
      "  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes==0.43.3) (2.4.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes==0.43.3) (1.26.4)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.3) (3.15.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.3) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.3) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.3) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.3) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.3) (2024.6.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes==0.43.3) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes==0.43.3) (1.3.0)\r\n",
      "Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\r\n",
      "Successfully installed bitsandbytes-0.43.3\r\n",
      "Collecting peft==0.12.0\r\n",
      "  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.12.0) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.12.0) (21.3)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.12.0) (5.9.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.12.0) (6.0.2)\r\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.12.0) (2.4.0)\r\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft==0.12.0) (4.45.1)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft==0.12.0) (4.66.4)\r\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.12.0) (0.33.0)\r\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.12.0) (0.4.5)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.12.0) (0.25.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.12.0) (3.15.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.12.0) (2024.6.1)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.12.0) (2.32.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.12.0) (4.12.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.12.0) (3.1.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.12.0) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.12.0) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.12.0) (3.1.4)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.12.0) (2024.5.15)\r\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.12.0) (0.20.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.12.0) (2.1.5)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.12.0) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.12.0) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.12.0) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.12.0) (2024.8.30)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.12.0) (1.3.0)\r\n",
      "Downloading peft-0.12.0-py3-none-any.whl (296 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: peft\r\n",
      "Successfully installed peft-0.12.0\r\n",
      "Collecting transformers==4.44.0\r\n",
      "  Downloading transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.44.0) (3.15.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers==4.44.0) (0.25.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.44.0) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.44.0) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.44.0) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.44.0) (2024.5.15)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.44.0) (2.32.3)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.44.0) (0.4.5)\r\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.44.0)\r\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.44.0) (4.66.4)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.0) (2024.6.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.0) (4.12.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.44.0) (3.1.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.44.0) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.44.0) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.44.0) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.44.0) (2024.8.30)\r\n",
      "Downloading transformers-4.44.0-py3-none-any.whl (9.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers\r\n",
      "  Attempting uninstall: tokenizers\r\n",
      "    Found existing installation: tokenizers 0.20.0\r\n",
      "    Uninstalling tokenizers-0.20.0:\r\n",
      "      Successfully uninstalled tokenizers-0.20.0\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.45.1\r\n",
      "    Uninstalling transformers-4.45.1:\r\n",
      "      Successfully uninstalled transformers-4.45.1\r\n",
      "Successfully installed tokenizers-0.19.1 transformers-4.44.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate==0.33.0\n",
    "!pip install bitsandbytes==0.43.3\n",
    "!pip install peft==0.12.0 \n",
    "!pip install transformers==4.44.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52c6abec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T15:47:03.747165Z",
     "iopub.status.busy": "2024-12-28T15:47:03.746876Z",
     "iopub.status.idle": "2024-12-28T15:47:03.751468Z",
     "shell.execute_reply": "2024-12-28T15:47:03.750642Z"
    },
    "papermill": {
     "duration": 0.014994,
     "end_time": "2024-12-28T15:47:03.753290",
     "exception": false,
     "start_time": "2024-12-28T15:47:03.738296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "VER=158\n",
    "\n",
    "# FINAL SOLUTION IS USE_QLORA=FALSE, TRAIN_100_PERCENT=TRUE, ADD_33K=TRUE, DEBUG=FALSE\n",
    "USE_QLORA = True\n",
    "TRAIN_100_PERCENT = False\n",
    "ADD_33K = False\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eff37b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T15:47:03.769034Z",
     "iopub.status.busy": "2024-12-28T15:47:03.768819Z",
     "iopub.status.idle": "2024-12-28T15:47:24.714667Z",
     "shell.execute_reply": "2024-12-28T15:47:24.713960Z"
    },
    "papermill": {
     "duration": 20.955787,
     "end_time": "2024-12-28T15:47:24.716707",
     "exception": false,
     "start_time": "2024-12-28T15:47:03.760920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    PreTrainedTokenizerBase, \n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from sklearn.metrics import log_loss, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "042bb95f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T15:47:24.732825Z",
     "iopub.status.busy": "2024-12-28T15:47:24.732312Z",
     "iopub.status.idle": "2024-12-28T15:47:24.738733Z",
     "shell.execute_reply": "2024-12-28T15:47:24.737975Z"
    },
    "papermill": {
     "duration": 0.01589,
     "end_time": "2024-12-28T15:47:24.740270",
     "exception": false,
     "start_time": "2024-12-28T15:47:24.724380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    output_dir: str = f\"output-{VER}\"\n",
    "    checkpoint: str = \"/kaggle/input/download-llama\"  \n",
    "    max_length: int = 2048\n",
    "    n_splits: int = 5\n",
    "    fold_idx: int = 0\n",
    "    optim_type: str = \"adamw_8bit\"\n",
    "    per_device_train_batch_size: int = 2\n",
    "    gradient_accumulation_steps: int = 4  # global batch size is 8 \n",
    "    per_device_eval_batch_size: int = 4\n",
    "    n_epochs: int = 1\n",
    "    freeze_layers: int = 0 # there're 42 layers in total, we don't add adapters to the first 16 layers\n",
    "    lr: float = 2e-4\n",
    "    warmup_steps: int = 20\n",
    "    lora_r: int = 64\n",
    "    lora_alpha: float = 4 \n",
    "    lora_dropout: float = 0.05\n",
    "    lora_bias: str = \"none\"\n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "143ad590",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T15:47:24.755452Z",
     "iopub.status.busy": "2024-12-28T15:47:24.755189Z",
     "iopub.status.idle": "2024-12-28T15:47:24.904878Z",
     "shell.execute_reply": "2024-12-28T15:47:24.904127Z"
    },
    "papermill": {
     "duration": 0.159355,
     "end_time": "2024-12-28T15:47:24.906878",
     "exception": false,
     "start_time": "2024-12-28T15:47:24.747523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = f\"output-{VER}\",\n",
    "    overwrite_output_dir=True,\n",
    "    report_to=\"none\",\n",
    "    num_train_epochs=config.n_epochs,\n",
    "    per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\", # don't save any checkpoints\n",
    "    #save_steps=200,\n",
    "    optim=config.optim_type,\n",
    "    fp16=True, \n",
    "    # bf16=True,\n",
    "    learning_rate=config.lr,\n",
    "    warmup_steps=config.warmup_steps,\n",
    "\n",
    "    #gradient_checkpointing=True, # this doesn't work correctly for some reason\n",
    "\n",
    "    #logging_first_step=True,\n",
    "    #lr_scheduler_type='linear', # \"cosine\" or \"linear\" or \"constant\" (default is linear)\n",
    "    metric_for_best_model='acc_classify',\n",
    "    greater_is_better=True,  \n",
    "    #save_total_limit=4,\n",
    "    #load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a9f15c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T15:47:24.923110Z",
     "iopub.status.busy": "2024-12-28T15:47:24.922860Z",
     "iopub.status.idle": "2024-12-28T15:47:24.927460Z",
     "shell.execute_reply": "2024-12-28T15:47:24.926820Z"
    },
    "papermill": {
     "duration": 0.0145,
     "end_time": "2024-12-28T15:47:24.929060",
     "exception": false,
     "start_time": "2024-12-28T15:47:24.914560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=config.lora_r,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    # only target self-attention\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\",\n",
    "                    \"down_proj\",\"up_proj\",\"o_proj\",\"gate_proj\"],\n",
    "    layers_to_transform=[i for i in range(32) if i >= config.freeze_layers],\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    bias=config.lora_bias,\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    modules_to_save=[\"score\",\"classifier_head1\", \"classifier_head2\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e7422ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T15:47:24.944246Z",
     "iopub.status.busy": "2024-12-28T15:47:24.944009Z",
     "iopub.status.idle": "2024-12-28T15:47:25.405915Z",
     "shell.execute_reply": "2024-12-28T15:47:25.405178Z"
    },
    "papermill": {
     "duration": 0.471948,
     "end_time": "2024-12-28T15:47:25.408102",
     "exception": false,
     "start_time": "2024-12-28T15:47:24.936154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizerFast\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.checkpoint, use_fast=True)\n",
    "tokenizer.add_eos_token = True  # We'll add <eos> at the end\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46c15f07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T15:47:25.424059Z",
     "iopub.status.busy": "2024-12-28T15:47:25.423788Z",
     "iopub.status.idle": "2024-12-28T15:47:25.430007Z",
     "shell.execute_reply": "2024-12-28T15:47:25.429300Z"
    },
    "papermill": {
     "duration": 0.016057,
     "end_time": "2024-12-28T15:47:25.431724",
     "exception": false,
     "start_time": "2024-12-28T15:47:25.415667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using QLoRA\n"
     ]
    }
   ],
   "source": [
    "qlora = {}\n",
    "if USE_QLORA:\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = True,\n",
    "        bnb_4bit_quant_type = \"nf4\", #nf4 or fp4\n",
    "        bnb_4bit_use_double_quant = False,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        llm_int8_skip_modules = [\"score\",\"classifier_head1\", \"classifier_head2\"]\n",
    "    )\n",
    "    qlora['quantization_config'] = bnb_config\n",
    "    print(\"Using QLoRA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9ff14f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T15:47:25.447594Z",
     "iopub.status.busy": "2024-12-28T15:47:25.447333Z",
     "iopub.status.idle": "2024-12-28T15:49:03.706973Z",
     "shell.execute_reply": "2024-12-28T15:49:03.706069Z"
    },
    "papermill": {
     "duration": 98.269756,
     "end_time": "2024-12-28T15:49:03.708932",
     "exception": false,
     "start_time": "2024-12-28T15:47:25.439176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae8b30e514c4430a97ca39ee457999f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CustomLlama3ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/download-llama and are newly initialized: ['classifier_head1.1.bias', 'classifier_head1.1.weight', 'classifier_head1.4.bias', 'classifier_head1.4.weight', 'classifier_head2.1.bias', 'classifier_head2.1.weight', 'classifier_head2.4.bias', 'classifier_head2.4.weight', 'score.1.bias', 'score.1.weight', 'score.4.bias', 'score.4.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): CustomLlama3ForSequenceClassification(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128257, 4096, padding_idx=128256)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Sequential(\n",
       "          (0): Dropout(p=0.1, inplace=False)\n",
       "          (1): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): GELU(approximate='none')\n",
       "          (4): Linear(in_features=2048, out_features=2, bias=True)\n",
       "        )\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Sequential(\n",
       "            (0): Dropout(p=0.1, inplace=False)\n",
       "            (1): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): GELU(approximate='none')\n",
       "            (4): Linear(in_features=2048, out_features=2, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (classifier_head1): ModulesToSaveWrapper(\n",
       "        (original_module): Sequential(\n",
       "          (0): Dropout(p=0.1, inplace=False)\n",
       "          (1): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): GELU(approximate='none')\n",
       "          (4): Linear(in_features=2048, out_features=126, bias=True)\n",
       "        )\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Sequential(\n",
       "            (0): Dropout(p=0.1, inplace=False)\n",
       "            (1): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): GELU(approximate='none')\n",
       "            (4): Linear(in_features=2048, out_features=126, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (classifier_head2): ModulesToSaveWrapper(\n",
       "        (original_module): Sequential(\n",
       "          (0): Dropout(p=0.1, inplace=False)\n",
       "          (1): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): GELU(approximate='none')\n",
       "          (4): Linear(in_features=2048, out_features=126, bias=True)\n",
       "        )\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Sequential(\n",
       "            (0): Dropout(p=0.1, inplace=False)\n",
       "            (1): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): GELU(approximate='none')\n",
       "            (4): Linear(in_features=2048, out_features=126, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomLlama3ForSequenceClassification(LlamaForSequenceClassification):\n",
    "    def __init__(self, config, num_labels_head1=60, num_labels_head2=60):\n",
    "        super().__init__(config)\n",
    "        self.num_labels_head1 = num_labels_head1\n",
    "        self.num_labels_head2 = num_labels_head2\n",
    "        hdim = config.hidden_size\n",
    "        self.classifier_head1 = torch.nn.Sequential(\n",
    "                                    torch.nn.Dropout(0.1),\n",
    "                                    torch.nn.Linear(hdim, hdim // 2),\n",
    "                                    torch.nn.Dropout(0.1),\n",
    "                                    torch.nn.GELU(),\n",
    "                                    torch.nn.Linear(hdim // 2, num_labels_head1),\n",
    "                                )\n",
    "        self.classifier_head2 = torch.nn.Sequential(\n",
    "                                    torch.nn.Dropout(0.1),\n",
    "                                    torch.nn.Linear(hdim, hdim // 2),\n",
    "                                    torch.nn.Dropout(0.1),\n",
    "                                    torch.nn.GELU(),\n",
    "                                    torch.nn.Linear(hdim // 2, num_labels_head2),\n",
    "                                )\n",
    "        self.score = torch.nn.Sequential(\n",
    "                            torch.nn.Dropout(0.1),\n",
    "                            torch.nn.Linear(hdim, hdim // 2),\n",
    "                            torch.nn.Dropout(0.1),\n",
    "                            torch.nn.GELU(),\n",
    "                            torch.nn.Linear(hdim // 2, 2),\n",
    "                        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):\n",
    "        device = input_ids.device\n",
    "\n",
    "        if labels is not None:\n",
    "            labels = labels.to(device)\n",
    "            outputs = super().forward(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        else:\n",
    "            outputs = super().forward(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        last_token_indices = (torch.sum(attention_mask, dim=1) - 1).to(device)\n",
    "        last_token_outputs = outputs.hidden_states[-1].to(device)[\n",
    "            torch.arange(outputs.hidden_states[-1].shape[0], device=device), last_token_indices]\n",
    "\n",
    "        outputs_head1 = self.classifier_head1(last_token_outputs).to(device)\n",
    "        outputs_head2 = self.classifier_head2(last_token_outputs).to(device)\n",
    "\n",
    "        if labels is not None:\n",
    "            labels_head1 = labels[:, 1].to(device)\n",
    "            labels_head2 = labels[:, 2].to(device)\n",
    "            \n",
    "            loss_head1 = nn.CrossEntropyLoss()(outputs_head1, labels_head1)\n",
    "            loss_head2 = nn.CrossEntropyLoss()(outputs_head2, labels_head2)\n",
    "            loss = nn.CrossEntropyLoss()(outputs.logits.to(device), labels[:, 0].to(device)).to(device) + 0.1 * loss_head1 + 0.1 * loss_head2\n",
    "            return {\"loss\": loss, \"logits\": (outputs.logits, outputs_head1, outputs_head2)}\n",
    "        else:\n",
    "            return {\"logits\": (outputs.logits, outputs_head1, outputs_head2)}\n",
    "\n",
    "config2 = AutoConfig.from_pretrained(config.checkpoint)\n",
    "model = CustomLlama3ForSequenceClassification.from_pretrained(\n",
    "    config.checkpoint,\n",
    "    config=config2,\n",
    "    num_labels_head1=126,\n",
    "    num_labels_head2=126,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    **qlora\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "# model.config.attn_logit_softcapping = None\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16e88cd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T15:49:03.730536Z",
     "iopub.status.busy": "2024-12-28T15:49:03.730249Z",
     "iopub.status.idle": "2024-12-28T15:49:03.740093Z",
     "shell.execute_reply": "2024-12-28T15:49:03.739229Z"
    },
    "papermill": {
     "duration": 0.02187,
     "end_time": "2024-12-28T15:49:03.741851",
     "exception": false,
     "start_time": "2024-12-28T15:49:03.719981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 193,464,574 || all params: 7,724,085,756 || trainable%: 2.5047\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a95ad585",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T15:49:03.757944Z",
     "iopub.status.busy": "2024-12-28T15:49:03.757698Z",
     "iopub.status.idle": "2024-12-28T15:49:05.899616Z",
     "shell.execute_reply": "2024-12-28T15:49:05.898468Z"
    },
    "papermill": {
     "duration": 2.152129,
     "end_time": "2024-12-28T15:49:05.901578",
     "exception": false,
     "start_time": "2024-12-28T15:49:03.749449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Competition data has shape (48439, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00007cff95d7f7974642a785aca248b0f26e60d3312fac...</td>\n",
       "      <td>vieš po Slovensky?</td>\n",
       "      <td>Áno, hovorím po slovensky. Ako vám môžem pomôcť?</td>\n",
       "      <td>Áno, veď som tu! Môžem ti pomôcť s otázkami al...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>o1-preview</td>\n",
       "      <td>reka-core-20240904</td>\n",
       "      <td>Slovak</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id              prompt  \\\n",
       "0  00007cff95d7f7974642a785aca248b0f26e60d3312fac...  vieš po Slovensky?   \n",
       "\n",
       "                                         response_a  \\\n",
       "0  Áno, hovorím po slovensky. Ako vám môžem pomôcť?   \n",
       "\n",
       "                                          response_b   winner     model_a  \\\n",
       "0  Áno, veď som tu! Môžem ti pomôcť s otázkami al...  model_a  o1-preview   \n",
       "\n",
       "              model_b language  \n",
       "0  reka-core-20240904   Slovak  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"/kaggle/input/wsdm-cup-multilingual-chatbot-arena/train.parquet\") \n",
    "df[\"id\"] = df[\"id\"].astype(\"str\")\n",
    "print('Competition data has shape', df.shape )\n",
    "LN = len(df)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1baad626",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T15:49:05.920638Z",
     "iopub.status.busy": "2024-12-28T15:49:05.919896Z",
     "iopub.status.idle": "2024-12-28T15:49:05.930630Z",
     "shell.execute_reply": "2024-12-28T15:49:05.929733Z"
    },
    "papermill": {
     "duration": 0.022286,
     "end_time": "2024-12-28T15:49:05.932771",
     "exception": false,
     "start_time": "2024-12-28T15:49:05.910485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use train data with shape (64, 8)\n"
     ]
    }
   ],
   "source": [
    "if ADD_33K:\n",
    "    df = pd.concat([df,df2],axis=0,ignore_index=True)\n",
    "if DEBUG:\n",
    "    df = df.iloc[:64].copy()\n",
    "print(\"We will use train data with shape\", df.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a54e17e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T15:49:05.950683Z",
     "iopub.status.busy": "2024-12-28T15:49:05.950022Z",
     "iopub.status.idle": "2024-12-28T15:49:05.978761Z",
     "shell.execute_reply": "2024-12-28T15:49:05.977859Z"
    },
    "papermill": {
     "duration": 0.039426,
     "end_time": "2024-12-28T15:49:05.980466",
     "exception": false,
     "start_time": "2024-12-28T15:49:05.941040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 47 unique models:\n",
      "{'chatgpt-4o-latest-20240808': 0, 'chatgpt-4o-latest-20240903': 1, 'claude-3-5-sonnet-20240620': 2, 'claude-3-5-sonnet-20241022': 3, 'claude-3-haiku-20240307': 4, 'claude-3-opus-20240229': 5, 'command-r-08-2024': 6, 'command-r-plus-08-2024': 7, 'deepseek-v2.5': 8, 'gemini-1.5-flash-002': 9, 'gemini-1.5-flash-8b-001': 10, 'gemini-1.5-flash-8b-exp-0827': 11, 'gemini-1.5-flash-exp-0827': 12, 'gemini-1.5-pro-001': 13, 'gemini-1.5-pro-002': 14, 'gemini-1.5-pro-exp-0827': 15, 'gemma-2-27b-it': 16, 'gemma-2-2b-it': 17, 'gemma-2-9b-it': 18, 'gpt-4-0125-preview': 19, 'gpt-4-1106-preview': 20, 'gpt-4-turbo-2024-04-09': 21, 'gpt-4o-2024-05-13': 22, 'gpt-4o-2024-08-06': 23, 'gpt-4o-mini-2024-07-18': 24, 'grok-2-2024-08-13': 25, 'grok-2-mini-2024-08-13': 26, 'internlm2_5-20b-chat': 27, 'jamba-1.5-mini': 28, 'llama-3.1-405b-instruct-bf16': 29, 'llama-3.1-405b-instruct-fp8': 30, 'llama-3.1-70b-instruct': 31, 'llama-3.1-8b-instruct': 32, 'llama-3.1-nemotron-70b-instruct': 33, 'llama-3.2-1b-instruct': 34, 'llama-3.2-3b-instruct': 35, 'mistral-large-2407': 36, 'o1-mini': 37, 'o1-preview': 38, 'qwen-max-0919': 39, 'qwen-plus-0828': 40, 'qwen2.5-72b-instruct': 41, 'reka-core-20240904': 42, 'reka-flash-20240722': 43, 'reka-flash-20240904': 44, 'yi-lightning': 45, 'yi-lightning-lite': 46}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00007cff95d7f7974642a785aca248b0f26e60d3312fac...</td>\n",
       "      <td>vieš po Slovensky?</td>\n",
       "      <td>Áno, hovorím po slovensky. Ako vám môžem pomôcť?</td>\n",
       "      <td>Áno, veď som tu! Môžem ti pomôcť s otázkami al...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>38</td>\n",
       "      <td>42</td>\n",
       "      <td>Slovak</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id              prompt  \\\n",
       "0  00007cff95d7f7974642a785aca248b0f26e60d3312fac...  vieš po Slovensky?   \n",
       "\n",
       "                                         response_a  \\\n",
       "0  Áno, hovorím po slovensky. Ako vám môžem pomôcť?   \n",
       "\n",
       "                                          response_b   winner  model_a  \\\n",
       "0  Áno, veď som tu! Môžem ti pomôcť s otázkami al...  model_a       38   \n",
       "\n",
       "   model_b language  \n",
       "0       42   Slovak  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "m1 = df.model_a.unique()\n",
    "m2 = df.model_b.unique()\n",
    "m = np.union1d(m1,m2)\n",
    "m = sorted(m)\n",
    "print(f\"There are {len(m)} unique models:\")\n",
    "\n",
    "MAP = {x:y for x,y in zip(m,range(len(m)))}\n",
    "print(MAP)\n",
    "\n",
    "df.model_a = df.model_a.map(MAP).astype('int32')\n",
    "df.model_b = df.model_b.map(MAP).astype('int32')\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edea4324",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T15:49:05.998659Z",
     "iopub.status.busy": "2024-12-28T15:49:05.998273Z",
     "iopub.status.idle": "2024-12-28T15:49:06.018767Z",
     "shell.execute_reply": "2024-12-28T15:49:06.017884Z"
    },
    "papermill": {
     "duration": 0.032246,
     "end_time": "2024-12-28T15:49:06.021003",
     "exception": false,
     "start_time": "2024-12-28T15:49:05.988757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed2142bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T15:49:06.050239Z",
     "iopub.status.busy": "2024-12-28T15:49:06.049967Z",
     "iopub.status.idle": "2024-12-28T15:49:06.068333Z",
     "shell.execute_reply": "2024-12-28T15:49:06.067461Z"
    },
    "papermill": {
     "duration": 0.034728,
     "end_time": "2024-12-28T15:49:06.070138",
     "exception": false,
     "start_time": "2024-12-28T15:49:06.035410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "class CustomTokenizer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        tokenizer: PreTrainedTokenizerBase, \n",
    "        max_length: int\n",
    "    ) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def prepare_text(self, prompts, responses_a, responses_b):\n",
    "        truncation_p_flag = False\n",
    "        origin_p = prompts\n",
    "        \n",
    "        instruction = '<|start_header_id|>Which response is better? [A or B]<|end_header_id|>\\nAnswer: '\n",
    "\n",
    "        # 去掉instruction的token, 去掉首尾<bos>和<eos>两个token\n",
    "        remain_len = self.max_length - len(self.tokenizer(instruction, add_special_tokens=False)['input_ids']) - 2\n",
    "\n",
    "        # 去掉<eos>和<bos>两个token，`<start_of_turn>prompt\\n<end_of_turn>\\n`一共占6个token\n",
    "        prompt_length = len(self.tokenizer(prompts)['input_ids']) + 6\n",
    "        origin_p_len = prompt_length\n",
    "        \n",
    "        if prompt_length > remain_len // 2:\n",
    "            prompt_length = remain_len // 2\n",
    "            prompts = '......' + self.tokenizer.decode(self.tokenizer(prompts)['input_ids'][-prompt_length + 7:])\n",
    "            truncation_p_flag = True\n",
    "        remain_len -= prompt_length\n",
    "        prompts = f\"<|start_header_id|>prompt<|end_header_id|>\\n{prompts}<|eot_id|>\\n\"\n",
    "\n",
    "        # <start_of_turn>response_b\\n<end_of_turn>\\n占7个token\n",
    "        response_a_length = len(self.tokenizer(responses_a)['input_ids']) + 7\n",
    "        response_b_length = len(self.tokenizer(responses_b)['input_ids']) + 7\n",
    "        \n",
    "        if response_a_length + response_b_length > remain_len:\n",
    "            # 按照模型输出长度比例分配\n",
    "            response_a_length = int(remain_len * response_a_length / (response_a_length + response_b_length))\n",
    "            response_b_length = int(remain_len * response_b_length / (response_a_length + response_b_length))\n",
    "            responses_a = '......' + self.tokenizer.decode(self.tokenizer(responses_a)['input_ids'][-response_a_length + 8:])\n",
    "            responses_b = '......' + self.tokenizer.decode(self.tokenizer(responses_b)['input_ids'][-response_b_length + 8:])\n",
    "        remain_len -= (response_a_length + response_b_length)\n",
    "\n",
    "        response_a = f\"<|start_header_id|>response_a<|end_header_id|>\\n{responses_a}<|eot_id|>\\n\"\n",
    "        response_b = f\"<|start_header_id|>response_a<|end_header_id|>\\n{responses_b}<|eot_id|>\\n\"\n",
    "\n",
    "        # 检测是不是还有空间给prompts\n",
    "        if remain_len > 0 and truncation_p_flag:\n",
    "            remain_len += prompt_length\n",
    "            if remain_len < origin_p_len:\n",
    "                prompts = '......' + self.tokenizer.decode(self.tokenizer(origin_p)['input_ids'][-remain_len + 7:])\n",
    "            else:\n",
    "                prompts = origin_p\n",
    "            prompts = f\"<|start_header_id|>prompt<|end_header_id|>\\n{prompts}<|eot_id|>\\n\"     \n",
    "\n",
    "        return '<|begin_of_text|>' + prompts + response_a + response_b + instruction + '<|end_of_text|>'\n",
    "\n",
    "        \n",
    "    def __call__(self, batch: dict) -> dict:\n",
    "        \n",
    "        texts = [\n",
    "            self.prepare_text(p, r_a, r_b)\n",
    "            for p, r_a, r_b in zip(batch[\"prompt\"], batch[\"response_a\"], batch[\"response_b\"])\n",
    "        ]\n",
    "        \n",
    "        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)\n",
    "        labels=[]\n",
    "        for win, c, d in zip(batch[\"winner\"], \n",
    "                                   batch[\"model_a\"],batch[\"model_b\"]):\n",
    "            if win == 'model_a':\n",
    "                label = 0\n",
    "            elif win == 'model_b':\n",
    "                label = 1\n",
    "            labels.append( (label,c,d) )\n",
    "        return {**tokenized, \"labels\": labels} #, \"texts\": texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d0d390d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T15:49:06.097811Z",
     "iopub.status.busy": "2024-12-28T15:49:06.097536Z",
     "iopub.status.idle": "2024-12-28T15:49:09.889546Z",
     "shell.execute_reply": "2024-12-28T15:49:09.888614Z"
    },
    "papermill": {
     "duration": 3.80403,
     "end_time": "2024-12-28T15:49:09.891524",
     "exception": false,
     "start_time": "2024-12-28T15:49:06.087494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b06ebfbe1f4d6a8ae31ad6f8dbfcfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/64 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encode = CustomTokenizer(tokenizer, max_length=config.max_length)\n",
    "ds = ds.map(encode, batched=True, num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55f0a86c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T15:49:09.909724Z",
     "iopub.status.busy": "2024-12-28T15:49:09.909413Z",
     "iopub.status.idle": "2024-12-28T15:49:09.917447Z",
     "shell.execute_reply": "2024-12-28T15:49:09.916487Z"
    },
    "papermill": {
     "duration": 0.019179,
     "end_time": "2024-12-28T15:49:09.919341",
     "exception": false,
     "start_time": "2024-12-28T15:49:09.900162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds: EvalPrediction) -> dict:\n",
    "    preds = eval_preds.predictions\n",
    "    labels = np.array( eval_preds.label_ids )\n",
    "    \n",
    "    # Split the predictions and labels into two heads\n",
    "    preds_head1 = preds[0]\n",
    "    preds_head2 = preds[1]\n",
    "    preds_head3 = preds[2]\n",
    "    labels_head1 = labels[:,0]\n",
    "    labels_head2 = labels[:,1]\n",
    "    labels_head3 = labels[:,2]\n",
    "    \n",
    "    # Compute log loss and accuracy for each head\n",
    "    probs_head1 = torch.from_numpy(preds_head1).float().softmax(-1).numpy()\n",
    "    loss_head1 = log_loss(y_true=labels_head1, y_pred=probs_head1, labels=[x for x in range(2)])\n",
    "    acc_head1 = accuracy_score(y_true=labels_head1, y_pred=preds_head1.argmax(-1))\n",
    "    \n",
    "    probs_head2 = torch.from_numpy(preds_head2).float().softmax(-1).numpy()\n",
    "    loss_head2 = log_loss(y_true=labels_head2, y_pred=probs_head2, labels=[x for x in range(126)])\n",
    "    acc_head2 = accuracy_score(y_true=labels_head2, y_pred=preds_head2.argmax(-1))\n",
    "\n",
    "    probs_head3 = torch.from_numpy(preds_head3).float().softmax(-1).numpy()\n",
    "    loss_head3 = log_loss(y_true=labels_head3, y_pred=probs_head3, labels=[x for x in range(126)])\n",
    "    acc_head3 = accuracy_score(y_true=labels_head3, y_pred=preds_head3.argmax(-1))\n",
    "    \n",
    "    # Return the metrics for each head\n",
    "    return {\n",
    "        \"acc_classify\": acc_head1,\n",
    "        \"log_loss_classify\": loss_head1,\n",
    "        \"acc_model_a\": acc_head2,\n",
    "        \"log_loss_model_a\": loss_head2,\n",
    "        \"acc_model_b\": acc_head3,\n",
    "        \"log_loss_model_b\": loss_head3\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10b6b4dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T15:49:09.936818Z",
     "iopub.status.busy": "2024-12-28T15:49:09.936574Z",
     "iopub.status.idle": "2024-12-28T15:49:09.942034Z",
     "shell.execute_reply": "2024-12-28T15:49:09.941331Z"
    },
    "papermill": {
     "duration": 0.015937,
     "end_time": "2024-12-28T15:49:09.943531",
     "exception": false,
     "start_time": "2024-12-28T15:49:09.927594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if TRAIN_100_PERCENT:\n",
    "    folds = [\n",
    "        (\n",
    "            [i for i in range(len(ds))], \n",
    "            [i for i in range(len(ds)) if (i % config.n_splits == fold_idx)&(i<LN)]\n",
    "        ) \n",
    "        for fold_idx in range(config.n_splits)\n",
    "    ]\n",
    "    print(\"We are training with 100% data\")\n",
    "else:\n",
    "    folds = [\n",
    "        (\n",
    "            [i for i in range(len(ds)) if i % config.n_splits != fold_idx],\n",
    "            [i for i in range(len(ds)) if (i % config.n_splits == fold_idx)&(i<LN)]\n",
    "        ) \n",
    "        for fold_idx in range(config.n_splits)\n",
    "    ]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a59febdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T15:49:09.960773Z",
     "iopub.status.busy": "2024-12-28T15:49:09.960512Z",
     "iopub.status.idle": "2024-12-28T15:54:25.509463Z",
     "shell.execute_reply": "2024-12-28T15:54:25.508645Z"
    },
    "papermill": {
     "duration": 315.559395,
     "end_time": "2024-12-28T15:54:25.511080",
     "exception": false,
     "start_time": "2024-12-28T15:49:09.951685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 04:28, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc Classify</th>\n",
       "      <th>Log Loss Classify</th>\n",
       "      <th>Acc Model A</th>\n",
       "      <th>Log Loss Model A</th>\n",
       "      <th>Acc Model B</th>\n",
       "      <th>Log Loss Model B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.451444</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>1.254925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.004848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.959599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=2.258405844370524, metrics={'train_runtime': 314.1435, 'train_samples_per_second': 0.162, 'train_steps_per_second': 0.019, 'total_flos': 2873940602395152.0, 'train_loss': 2.258405844370524, 'epoch': 0.9230769230769231})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx, eval_idx = folds[config.fold_idx]\n",
    "\n",
    "trainer = Trainer(\n",
    "    args=training_args, \n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=ds.select(train_idx),\n",
    "    eval_dataset=ds.select(eval_idx),\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52a8e764",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T15:54:25.529560Z",
     "iopub.status.busy": "2024-12-28T15:54:25.529240Z",
     "iopub.status.idle": "2024-12-28T15:54:27.408023Z",
     "shell.execute_reply": "2024-12-28T15:54:27.407274Z"
    },
    "papermill": {
     "duration": 1.8904,
     "end_time": "2024-12-28T15:54:27.410077",
     "exception": false,
     "start_time": "2024-12-28T15:54:25.519677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.save_model(f\"LoRA-v{VER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d153e754",
   "metadata": {
    "papermill": {
     "duration": 0.008448,
     "end_time": "2024-12-28T15:54:27.427542",
     "exception": false,
     "start_time": "2024-12-28T15:54:27.419094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 10131489,
     "sourceId": 86946,
     "sourceType": "competition"
    },
    {
     "sourceId": 215033530,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 499.919217,
   "end_time": "2024-12-28T15:54:30.365553",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-28T15:46:10.446336",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0349c53680534432b8ea3c368c59a12c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "08a536f7f70c4d358b365e312ab44c2b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2b4b2f4bd04f449f94fcf92ab818feab": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2f393933f2a4435681524d437cc38558": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "34ec6ec920964335815e982e0229a16b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "44816787883f4c8aa23b1415adfc490d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4ce66b83db574b86ab5a1126de2f2d24": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c9e09b13174b4a6fa6a6fb2a7db6be0b",
       "placeholder": "​",
       "style": "IPY_MODEL_c2b3c89d091e4ed081081fbcd12b0269",
       "value": "Map (num_proc=8): 100%"
      }
     },
     "54877ef48a6744799ff4a030cf203baf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_bb9fe2e177434a999a3a1ea0275aa761",
       "placeholder": "​",
       "style": "IPY_MODEL_08a536f7f70c4d358b365e312ab44c2b",
       "value": " 64/64 [00:03&lt;00:00, 32.68 examples/s]"
      }
     },
     "73ad592de24946a19088e540dbc4276e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7ebf3e8500aa4711bceaad319c5c3ff5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7fbe9da79f164f96822c9b63b6537cfb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ae8b30e514c4430a97ca39ee457999f8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ce54f1728e1f42ae9cf89e33a6b9555c",
        "IPY_MODEL_fc20f0bbf44040249993eee85b568b96",
        "IPY_MODEL_d15db8ebcb4741a2b03505664af3e762"
       ],
       "layout": "IPY_MODEL_ba3a99cc570b49b99e909e9380d2a14f"
      }
     },
     "ba3a99cc570b49b99e909e9380d2a14f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bb9fe2e177434a999a3a1ea0275aa761": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c2b3c89d091e4ed081081fbcd12b0269": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c9e09b13174b4a6fa6a6fb2a7db6be0b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ce54f1728e1f42ae9cf89e33a6b9555c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2f393933f2a4435681524d437cc38558",
       "placeholder": "​",
       "style": "IPY_MODEL_7fbe9da79f164f96822c9b63b6537cfb",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "d15db8ebcb4741a2b03505664af3e762": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_34ec6ec920964335815e982e0229a16b",
       "placeholder": "​",
       "style": "IPY_MODEL_73ad592de24946a19088e540dbc4276e",
       "value": " 4/4 [01:34&lt;00:00, 19.14s/it]"
      }
     },
     "d4b06ebfbe1f4d6a8ae31ad6f8dbfcfa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4ce66b83db574b86ab5a1126de2f2d24",
        "IPY_MODEL_fcff9f3318c44c2ab066e0c12ad1d579",
        "IPY_MODEL_54877ef48a6744799ff4a030cf203baf"
       ],
       "layout": "IPY_MODEL_44816787883f4c8aa23b1415adfc490d"
      }
     },
     "dd6b5186706f4397b2cdb9d9521589b4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "fc20f0bbf44040249993eee85b568b96": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2b4b2f4bd04f449f94fcf92ab818feab",
       "max": 4.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_dd6b5186706f4397b2cdb9d9521589b4",
       "value": 4.0
      }
     },
     "fcff9f3318c44c2ab066e0c12ad1d579": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7ebf3e8500aa4711bceaad319c5c3ff5",
       "max": 64.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_0349c53680534432b8ea3c368c59a12c",
       "value": 64.0
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
